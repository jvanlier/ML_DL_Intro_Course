{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 1_neural_network_classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PhVxFBTV6MwV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural Network Classification\n",
        "========================\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Run each cell from top to bottom. Try to understand the output of each command. If in doubt, ask your neighbours or  Jori."
      ]
    },
    {
      "metadata": {
        "id": "MqfvuI1N6MwY",
        "colab_type": "code",
        "outputId": "a0e7d83c-8679-4287-9f9d-7a76a1aa2889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.optimizers import Adam\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2zG5ozpi6Mwc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will load a dataset from the US census bureau. We are going to predict whether a person makes more than $ 50K a year, or less. "
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nctAund16Mwd",
        "colab_type": "code",
        "outputId": "d644a734-6b1d-4a92-91b1-19508af7dc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jvanlier/TIAS_ML_DL/master/Day2Notebooks/data/census.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>more-than-50k</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age          workclass   education  education-num       marital-status  \\\n",
              "0   39          State-gov   Bachelors             13        Never-married   \n",
              "1   50   Self-emp-not-inc   Bachelors             13   Married-civ-spouse   \n",
              "2   38            Private     HS-grad              9             Divorced   \n",
              "3   53            Private        11th              7   Married-civ-spouse   \n",
              "4   28            Private   Bachelors             13   Married-civ-spouse   \n",
              "\n",
              "           occupation    relationship    race      sex  capital-gain  \\\n",
              "0        Adm-clerical   Not-in-family   White     Male          2174   \n",
              "1     Exec-managerial         Husband   White     Male             0   \n",
              "2   Handlers-cleaners   Not-in-family   White     Male             0   \n",
              "3   Handlers-cleaners         Husband   Black     Male             0   \n",
              "4      Prof-specialty            Wife   Black   Female             0   \n",
              "\n",
              "   capital-loss  hours-per-week  native-country  more-than-50k  \n",
              "0             0              40   United-States              0  \n",
              "1             0              13   United-States              0  \n",
              "2             0              40   United-States              0  \n",
              "3             0              40   United-States              0  \n",
              "4             0              40            Cuba              0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "ARVzpJl7IIFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "expected = 32561\n",
        "assert len(df) == expected, f\"expected {expected} rows but got {len(df)}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDcu-Ze3DVTG",
        "colab_type": "code",
        "outputId": "0fc4c482-deb9-4de4-c503-2fbf050614b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "HhW3VWcK6Mwg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below are some more details about this dataset:\n",
        "\n",
        "\n",
        "- `age`: continuous.\n",
        "- `workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "- `education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, - 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "- `education-num`: continuous.\n",
        "- `marital-status`: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "- `occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "- `relationship`: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "- `race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
        "- `sex`: Female, Male.\n",
        "- `capital-gain`: continuous.\n",
        "- `capital-loss`: continuous.\n",
        "- `hours-per-week`: continuous.\n",
        "- `native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
        "\n",
        "Note that there are many textual columns, which are somewhat annoying to deal with.\n",
        "\n",
        "Our target is column `more-than-50k`"
      ]
    },
    {
      "metadata": {
        "id": "-i8sOPr16Mwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Luckily, we can use `pd.get_dummies` to OneHotEncode the dataset easily:"
      ]
    },
    {
      "metadata": {
        "id": "C7KgvXTR6Mwh",
        "colab_type": "code",
        "outputId": "2e10f1ed-8f36-4519-e5c4-c382fd4e2508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "df_ohe = pd.get_dummies(df, drop_first=True)\n",
        "df_ohe.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>education-num</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>more-than-50k</th>\n",
              "      <th>workclass_ Federal-gov</th>\n",
              "      <th>workclass_ Local-gov</th>\n",
              "      <th>workclass_ Never-worked</th>\n",
              "      <th>workclass_ Private</th>\n",
              "      <th>...</th>\n",
              "      <th>native-country_ Portugal</th>\n",
              "      <th>native-country_ Puerto-Rico</th>\n",
              "      <th>native-country_ Scotland</th>\n",
              "      <th>native-country_ South</th>\n",
              "      <th>native-country_ Taiwan</th>\n",
              "      <th>native-country_ Thailand</th>\n",
              "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
              "      <th>native-country_ United-States</th>\n",
              "      <th>native-country_ Vietnam</th>\n",
              "      <th>native-country_ Yugoslavia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
              "0   39             13          2174             0              40   \n",
              "1   50             13             0             0              13   \n",
              "2   38              9             0             0              40   \n",
              "3   53              7             0             0              40   \n",
              "4   28             13             0             0              40   \n",
              "\n",
              "   more-than-50k  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
              "0              0                       0                     0   \n",
              "1              0                       0                     0   \n",
              "2              0                       0                     0   \n",
              "3              0                       0                     0   \n",
              "4              0                       0                     0   \n",
              "\n",
              "   workclass_ Never-worked  workclass_ Private             ...              \\\n",
              "0                        0                   0             ...               \n",
              "1                        0                   0             ...               \n",
              "2                        0                   1             ...               \n",
              "3                        0                   1             ...               \n",
              "4                        0                   1             ...               \n",
              "\n",
              "   native-country_ Portugal  native-country_ Puerto-Rico  \\\n",
              "0                         0                            0   \n",
              "1                         0                            0   \n",
              "2                         0                            0   \n",
              "3                         0                            0   \n",
              "4                         0                            0   \n",
              "\n",
              "   native-country_ Scotland  native-country_ South  native-country_ Taiwan  \\\n",
              "0                         0                      0                       0   \n",
              "1                         0                      0                       0   \n",
              "2                         0                      0                       0   \n",
              "3                         0                      0                       0   \n",
              "4                         0                      0                       0   \n",
              "\n",
              "   native-country_ Thailand  native-country_ Trinadad&Tobago  \\\n",
              "0                         0                                0   \n",
              "1                         0                                0   \n",
              "2                         0                                0   \n",
              "3                         0                                0   \n",
              "4                         0                                0   \n",
              "\n",
              "   native-country_ United-States  native-country_ Vietnam  \\\n",
              "0                              1                        0   \n",
              "1                              1                        0   \n",
              "2                              1                        0   \n",
              "3                              1                        0   \n",
              "4                              0                        0   \n",
              "\n",
              "   native-country_ Yugoslavia  \n",
              "0                           0  \n",
              "1                           0  \n",
              "2                           0  \n",
              "3                           0  \n",
              "4                           0  \n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "co0nWf2zDfhP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6p3upuJ26dgP",
        "colab_type": "code",
        "outputId": "fd454e47-db46-452f-ce64-0ece96142169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "cell_type": "code",
      "source": [
        "df_ohe.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'education-num', 'capital-gain', 'capital-loss',\n",
              "       'hours-per-week', 'more-than-50k', 'workclass_ Federal-gov',\n",
              "       'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private',\n",
              "       'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
              "       'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 11th',\n",
              "       'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th',\n",
              "       'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm',\n",
              "       'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate',\n",
              "       'education_ HS-grad', 'education_ Masters', 'education_ Preschool',\n",
              "       'education_ Prof-school', 'education_ Some-college',\n",
              "       'marital-status_ Married-AF-spouse',\n",
              "       'marital-status_ Married-civ-spouse',\n",
              "       'marital-status_ Married-spouse-absent',\n",
              "       'marital-status_ Never-married', 'marital-status_ Separated',\n",
              "       'marital-status_ Widowed', 'occupation_ Adm-clerical',\n",
              "       'occupation_ Armed-Forces', 'occupation_ Craft-repair',\n",
              "       'occupation_ Exec-managerial', 'occupation_ Farming-fishing',\n",
              "       'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct',\n",
              "       'occupation_ Other-service', 'occupation_ Priv-house-serv',\n",
              "       'occupation_ Prof-specialty', 'occupation_ Protective-serv',\n",
              "       'occupation_ Sales', 'occupation_ Tech-support',\n",
              "       'occupation_ Transport-moving', 'relationship_ Not-in-family',\n",
              "       'relationship_ Other-relative', 'relationship_ Own-child',\n",
              "       'relationship_ Unmarried', 'relationship_ Wife',\n",
              "       'race_ Asian-Pac-Islander', 'race_ Black', 'race_ Other', 'race_ White',\n",
              "       'sex_ Male', 'native-country_ Cambodia', 'native-country_ Canada',\n",
              "       'native-country_ China', 'native-country_ Columbia',\n",
              "       'native-country_ Cuba', 'native-country_ Dominican-Republic',\n",
              "       'native-country_ Ecuador', 'native-country_ El-Salvador',\n",
              "       'native-country_ England', 'native-country_ France',\n",
              "       'native-country_ Germany', 'native-country_ Greece',\n",
              "       'native-country_ Guatemala', 'native-country_ Haiti',\n",
              "       'native-country_ Holand-Netherlands', 'native-country_ Honduras',\n",
              "       'native-country_ Hong', 'native-country_ Hungary',\n",
              "       'native-country_ India', 'native-country_ Iran',\n",
              "       'native-country_ Ireland', 'native-country_ Italy',\n",
              "       'native-country_ Jamaica', 'native-country_ Japan',\n",
              "       'native-country_ Laos', 'native-country_ Mexico',\n",
              "       'native-country_ Nicaragua',\n",
              "       'native-country_ Outlying-US(Guam-USVI-etc)', 'native-country_ Peru',\n",
              "       'native-country_ Philippines', 'native-country_ Poland',\n",
              "       'native-country_ Portugal', 'native-country_ Puerto-Rico',\n",
              "       'native-country_ Scotland', 'native-country_ South',\n",
              "       'native-country_ Taiwan', 'native-country_ Thailand',\n",
              "       'native-country_ Trinadad&Tobago', 'native-country_ United-States',\n",
              "       'native-country_ Vietnam', 'native-country_ Yugoslavia'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "AFva7ewq6Mwk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the skew in the target:"
      ]
    },
    {
      "metadata": {
        "id": "Ntw78bOD6Mwk",
        "colab_type": "code",
        "outputId": "83a5d4c7-cea8-4cc0-e90d-8e48da0518db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "df_ohe[\"more-than-50k\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    24720\n",
              "1     7841\n",
              "Name: more-than-50k, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "9AvVTEqQ6Mwn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yes, it's fairly skewed with many more 0 instances than 1 instances. Let's use the F1 score this time, instead of Accuracy."
      ]
    },
    {
      "metadata": {
        "id": "oQddLYjoDpFW",
        "colab_type": "code",
        "outputId": "4d7381eb-43c2-42d4-dfdd-9b5392462e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "24720 / (24720 + 7841)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7591904425539756"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "MxctvGtb6Mwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train-test split"
      ]
    },
    {
      "metadata": {
        "id": "SJvA3s9a6Mwp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZwAJaHT6Mwr",
        "colab_type": "code",
        "outputId": "8d3309f1-87af-45a2-fe4c-2f480c321f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_ohe.drop(\"more-than-50k\", axis=1), \n",
        "    df_ohe[\"more-than-50k\"], \n",
        "    test_size=0.2, \n",
        "    random_state=0)\n",
        "print(f\"{len(X_train)} training instances and {len(X_test)} test instances.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26048 training instances and 6513 test instances.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylk9vdfE6Mwu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Baseline\n",
        "Before we start diving into Neural Nets, let's first try setting a baseline with Logistic Regression."
      ]
    },
    {
      "metadata": {
        "id": "uaZjitEU6Mwv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "lr = LogisticRegressionCV(scoring=\"f1\", max_iter=1000, cv=3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxVWGSFc6Mwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LogisticRegressionCV uses an internal cross-validation loop to find a good value for the regularization parameter. This, as we know by know, helps with the overfitting problem.\n",
        "\n",
        "As a warm-up, start with a fit on the training data. This should be familiar after last week!"
      ]
    },
    {
      "metadata": {
        "id": "F0MGQVHG6Mwz",
        "colab_type": "code",
        "outputId": "448c3adc-7781-4eee-940d-ec78c2983802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n",
              "           fit_intercept=True, intercept_scaling=1.0, max_iter=1000,\n",
              "           multi_class='warn', n_jobs=None, penalty='l2', random_state=0,\n",
              "           refit=True, scoring='f1', solver='lbfgs', tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "s7rEbFXo6Mw1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Validate the model on both train and test."
      ]
    },
    {
      "metadata": {
        "id": "mmLnjn2O6Mw2",
        "colab_type": "code",
        "outputId": "79f80450-777e-4840-872c-5af6c47bcd58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Train score\", lr.score(X_train, y_train))\n",
        "print(\"Test score\", lr.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train score 0.6574928977272728\n",
            "Test score 0.6572218382861091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score has changed. The scoring parameter is now used. This warning will disappear in version 0.22.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:1926: ChangedBehaviorWarning: The long-standing behavior to use the accuracy score has changed. The scoring parameter is now used. This warning will disappear in version 0.22.\n",
            "  ChangedBehaviorWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ga0Vse2F6Mw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It could also be useful to take a look at the confusion matrix. We discussed this last week. It contains the number of True Postives, False Positives, False Negatives and True Negatives.\n",
        "\n",
        "Take a look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
      ]
    },
    {
      "metadata": {
        "id": "azQv7o2q6Mw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: import confusion matrix (hint: see top of documentation page for import path)\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sdr8Xvw16Mw9",
        "colab_type": "code",
        "outputId": "b58cc35d-bd6e-4ac8-ea07-a8538f5b4d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: create confusion matrix (hint: you need to pass it predictions on X_test)\n",
        "confusion_matrix(y_test, lr.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4570,  348],\n",
              "       [ 644,  951]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "_M9btdjB6Mw_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How many True Negatives do you have? And how many True Positives? Refer to the documentation to find out what each cell in the confusion matrix means."
      ]
    },
    {
      "metadata": {
        "id": "cb3J5dQf6MxA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "Let's now try to improve on this result by moving towards a more complex model.\n",
        "\n",
        "`scikit-learn` provides a basic neural network, but it's not really used a lot. Most practitioners prefer Keras or PyTorch. We'll use Keras, which is backed by Google."
      ]
    },
    {
      "metadata": {
        "id": "uRakTci76MxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N84VYu3x6MxD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll start by making a network that mimicks Logistic Regression."
      ]
    },
    {
      "metadata": {
        "id": "50DO1tzo6MxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(1)  # Leave this here! It ensures reproducability of results.\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_yPj2sr36MxH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we need to add just the single sigmoid node. In Keras terminology, this is a `Dense` layer, with a single unit.\n",
        "\n",
        "To initialize a dense layer, with 1 unit, `m` input features, and sigmoid activation function, use the following:\n",
        "\n",
        "`dense = Dense(1, input_dim=m, activation=\"sigmoid\")`\n",
        "\n",
        "To figure out what `m` is, you may use `X_train.shape` or `len(X_train.columns)`."
      ]
    },
    {
      "metadata": {
        "id": "g58OGgMG_LcM",
        "colab_type": "code",
        "outputId": "a867b271-5e3d-4fec-d180-e07efa2effc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "kJTFtNKp6MxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: create dense layer\n",
        "dense = Dense(1, input_dim=X_train.shape[1], activation=\"sigmoid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9X2BQyR6MxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now add your layer to the model using `model.add(...)`:"
      ]
    },
    {
      "metadata": {
        "id": "4LZqdmqw6MxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: add dense layer to model\n",
        "model.add(dense)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MLpRzNi26MxQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following command tells Keras how to optimize and evaluate the model. Unfortunately, there is no easy way to show F1 score during optimization, so will check it afterwards. \n",
        "\n",
        "You hopefully remember *binary crossentropy* from last week?"
      ]
    },
    {
      "metadata": {
        "id": "u92iGwBu6MxR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",  optimizer=Adam(lr=0.0003))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LU4n5QCb_ad8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Zd9JI2_56MxT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following command starts training the neural network."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "iDlVd_516MxT",
        "colab_type": "code",
        "outputId": "ab83b2e9-8cbc-4acb-cf2e-9f26de23d212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1730
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=50, batch_size=200, validation_data=[X_test, y_test])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 26048 samples, validate on 6513 samples\n",
            "Epoch 1/50\n",
            "26048/26048 [==============================] - 1s 24us/step - loss: 9.6853 - val_loss: 8.3605\n",
            "Epoch 2/50\n",
            "26048/26048 [==============================] - 0s 11us/step - loss: 7.0841 - val_loss: 5.6128\n",
            "Epoch 3/50\n",
            "26048/26048 [==============================] - 0s 11us/step - loss: 4.3776 - val_loss: 3.0200\n",
            "Epoch 4/50\n",
            "26048/26048 [==============================] - 0s 12us/step - loss: 2.1878 - val_loss: 1.5049\n",
            "Epoch 5/50\n",
            "26048/26048 [==============================] - 0s 10us/step - loss: 1.4262 - val_loss: 1.2912\n",
            "Epoch 6/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 1.3449 - val_loss: 1.2695\n",
            "Epoch 7/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 1.3317 - val_loss: 1.2609\n",
            "Epoch 8/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 1.3236 - val_loss: 1.2529\n",
            "Epoch 9/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 1.3156 - val_loss: 1.2449\n",
            "Epoch 10/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 1.3076 - val_loss: 1.2368\n",
            "Epoch 11/50\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 1.2987 - val_loss: 1.2194\n",
            "Epoch 12/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.8774 - val_loss: 0.7908\n",
            "Epoch 13/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.8068 - val_loss: 0.7745\n",
            "Epoch 14/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.8003 - val_loss: 0.7630\n",
            "Epoch 15/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7910 - val_loss: 0.7556\n",
            "Epoch 16/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7839 - val_loss: 0.7474\n",
            "Epoch 17/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7796 - val_loss: 0.7406\n",
            "Epoch 18/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7813 - val_loss: 0.7337\n",
            "Epoch 19/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7661 - val_loss: 0.7282\n",
            "Epoch 20/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7589 - val_loss: 0.7225\n",
            "Epoch 21/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7531 - val_loss: 0.7183\n",
            "Epoch 22/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7482 - val_loss: 0.7117\n",
            "Epoch 23/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7450 - val_loss: 0.7201\n",
            "Epoch 24/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7419 - val_loss: 0.7045\n",
            "Epoch 25/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7364 - val_loss: 0.6996\n",
            "Epoch 26/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7336 - val_loss: 0.7045\n",
            "Epoch 27/50\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.7291 - val_loss: 0.6974\n",
            "Epoch 28/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7253 - val_loss: 0.6912\n",
            "Epoch 29/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7226 - val_loss: 0.6918\n",
            "Epoch 30/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.7210 - val_loss: 0.6835\n",
            "Epoch 31/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.6995 - val_loss: 0.4945\n",
            "Epoch 32/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3756 - val_loss: 0.3744\n",
            "Epoch 33/50\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.3653 - val_loss: 0.3702\n",
            "Epoch 34/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3592 - val_loss: 0.3707\n",
            "Epoch 35/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3579 - val_loss: 0.3643\n",
            "Epoch 36/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3563 - val_loss: 0.3633\n",
            "Epoch 37/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3543 - val_loss: 0.3674\n",
            "Epoch 38/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3537 - val_loss: 0.3587\n",
            "Epoch 39/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3535 - val_loss: 0.3623\n",
            "Epoch 40/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3494 - val_loss: 0.3592\n",
            "Epoch 41/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3501 - val_loss: 0.3599\n",
            "Epoch 42/50\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.3478 - val_loss: 0.3599\n",
            "Epoch 43/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3472 - val_loss: 0.3564\n",
            "Epoch 44/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3484 - val_loss: 0.3563\n",
            "Epoch 45/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3433 - val_loss: 0.3524\n",
            "Epoch 46/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3430 - val_loss: 0.3522\n",
            "Epoch 47/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3431 - val_loss: 0.3547\n",
            "Epoch 48/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3416 - val_loss: 0.3489\n",
            "Epoch 49/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3422 - val_loss: 0.3510\n",
            "Epoch 50/50\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3415 - val_loss: 0.3476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd6fb9420b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "Io_PrPor6MxV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "50 epochs should get you a validation loss of approximately 0.35."
      ]
    },
    {
      "metadata": {
        "id": "Vk7B45Q36MxW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, now let's see the F1 score. First, we have to get predicted classes. Use `model.predict_classes(...)`."
      ]
    },
    {
      "metadata": {
        "id": "bZQGILKQ6MxX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_hat = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJ6KvM-j6MxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3yV5dPj6Mxa",
        "colab_type": "code",
        "outputId": "99496ef0-6f41-4386-c6bf-41099607ba72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "f1_score(y_test, y_test_hat)\n",
        "# YOUR CODE HERE: use f1_score function"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6271929824561403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "ZLzntb5j6Mxd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How do you feel about this F1 score? Disappointing?\n",
        "\n",
        "Well, that's as expected!\n",
        "\n",
        "The Logistic Regression implementation in scikit-learn uses a very sophisticated optimizer (L-BFGS). Neural Networks use less sophisticated optimizers (backpropagation with gradient descent), which makes them harder to train. You need to get many things right: e.g. number of epochs, batch size and learning rate. However, the less sophisticated optimizer used in Neural Networks *does* allows us to do backpropagation and update hidden layers, which we shall do soon!\n",
        "\n",
        "But first, try running the `fit()` method again and see if this improves the F1 score? In contrast to scikit-learn, repeated calls to this `fit()` method do not overwrite the previous model, but in fact continue training! You may run this command multiple times, until you no longer see `val_loss` improving."
      ]
    },
    {
      "metadata": {
        "id": "lSHwx_rZ6Mxe",
        "colab_type": "code",
        "outputId": "7e6d8b01-d39e-4c41-9fa3-a3c01f054705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: Run fit() again with 20 epochs. Keep everything else the same. Feel free to copy paste the command!\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=200, validation_data=[X_test, y_test])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 26048 samples, validate on 6513 samples\n",
            "Epoch 1/20\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.3316 - val_loss: 0.3422\n",
            "Epoch 2/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3317 - val_loss: 0.3460\n",
            "Epoch 3/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3327 - val_loss: 0.3399\n",
            "Epoch 4/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3437 - val_loss: 0.3498\n",
            "Epoch 5/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3335 - val_loss: 0.3417\n",
            "Epoch 6/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3312 - val_loss: 0.3429\n",
            "Epoch 7/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3321 - val_loss: 0.3518\n",
            "Epoch 8/20\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.3314 - val_loss: 0.3445\n",
            "Epoch 9/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3297 - val_loss: 0.3403\n",
            "Epoch 10/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3306 - val_loss: 0.3414\n",
            "Epoch 11/20\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.3307 - val_loss: 0.3544\n",
            "Epoch 12/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3303 - val_loss: 0.3415\n",
            "Epoch 13/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3307 - val_loss: 0.3407\n",
            "Epoch 14/20\n",
            "26048/26048 [==============================] - 0s 8us/step - loss: 0.3304 - val_loss: 0.3388\n",
            "Epoch 15/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3312 - val_loss: 0.3385\n",
            "Epoch 16/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3296 - val_loss: 0.3405\n",
            "Epoch 17/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3308 - val_loss: 0.3383\n",
            "Epoch 18/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3324 - val_loss: 0.3433\n",
            "Epoch 19/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3294 - val_loss: 0.3410\n",
            "Epoch 20/20\n",
            "26048/26048 [==============================] - 0s 9us/step - loss: 0.3296 - val_loss: 0.3393\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd6fd865da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "U8QrWpiI6Mxg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What is the F1 score now? Did training longer improve things?"
      ]
    },
    {
      "metadata": {
        "id": "zV8HFHrd6Mxi",
        "colab_type": "code",
        "outputId": "0d4e4d46-cd01-4e0e-cc61-dc9c6f85375c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE: what is the F1 score now? \n",
        "y_test_hat = model.predict_classes(X_test)\n",
        "f1_score(y_test, y_test_hat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6570048309178744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "8ALY6-R0GPag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Dense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69Z20Kul6Mxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Going deeper"
      ]
    },
    {
      "metadata": {
        "id": "ii8QYgXS6Mxk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, build a new neural network with a Dense hidden layer. It is defined much like before, although now, use 200 nodes instead of 1, and use tanh activation function instead of sigmoid:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "8CMbwu706Mxl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "model2 = Sequential()\n",
        "hidden_layer = Dense(200, input_dim=99, activation=\"tanh\")\n",
        "model2.add(hidden_layer)\n",
        "model2.add(Dense(1, activation=\"sigmoid\"))\n",
        "model2.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0003))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "CyMxg38B6Mxn",
        "colab_type": "code",
        "outputId": "ff3cf5c7-208d-48d5-b3fb-510f91522d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "model2.fit(X_train, y_train, epochs=10, batch_size=200, validation_data=[X_test, y_test])\n",
        "f1_score(y_test, model2.predict_classes(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 26048 samples, validate on 6513 samples\n",
            "Epoch 1/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3190 - val_loss: 0.3325\n",
            "Epoch 2/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3195 - val_loss: 0.3314\n",
            "Epoch 3/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3181 - val_loss: 0.3311\n",
            "Epoch 4/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3181 - val_loss: 0.3320\n",
            "Epoch 5/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3186 - val_loss: 0.3336\n",
            "Epoch 6/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3179 - val_loss: 0.3325\n",
            "Epoch 7/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3179 - val_loss: 0.3312\n",
            "Epoch 8/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3178 - val_loss: 0.3322\n",
            "Epoch 9/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3174 - val_loss: 0.3311\n",
            "Epoch 10/10\n",
            "26048/26048 [==============================] - 0s 17us/step - loss: 0.3169 - val_loss: 0.3342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6837294332723949"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "pOMuM3xk6Mxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should be able to get .67 - .68 after around 30 epochs. Run the cell above a couple of times. This a small - but not insignificant - improvement over simple Logistic Regression!"
      ]
    },
    {
      "metadata": {
        "id": "QGqg-3-A6Mxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Open ended bonus assignments"
      ]
    },
    {
      "metadata": {
        "id": "FSgVsa5x6Mxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Add a second hidden layer. Can you improve the score?\n",
        "- Try tuning the learning rate, batch size, number of hidden nodes. What is the best F1 score you can get?\n",
        "- Try training a Random Forest like we did last week. Feel free to copy-paste the appropriate bits of code from that notebook. How does the Random Forest compare to the Neural Network?"
      ]
    }
  ]
}